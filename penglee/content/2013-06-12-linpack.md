Title:高性能计算机系统浮点性能测试
Date:2013-06-12
Author:李鹏
Slug:linpack
Tags:benchmark
category:性能测试


<img src="https://d2lm6fxwu08ot6.cloudfront.net/img-thumbs/280h/P5LS1TUY3V.jpg" height="280" width="420">

### 一　工具介绍

Linpack现在在国际上已经成为最流行的用于测试高性能计算机系统浮点性能的benchmark。通过利用高性能计算机，用高斯消元法求解N元一次稠密线性代数方程组的测试，评价高性能计算机的浮点性能。HPL是针对现代并行计算机提出的测试方式。用户在不修改任意测试程序的基础上，可以调节问题规模大小N(矩阵大小)、使用到的CPU数目、使用各种优化方法等来执行该测试程序，以获取最佳的性能。HPL采用高斯消元法求解线性方程组。当求解问题规模为N时，浮点运算次数为(2/3 * N^3－2*N^2)。因此，只要给出问题规模N，测得系统计算时间T，峰值=计算量(2/3 * N^3－2*N^2)/计算时间T，测试结果以浮点运算每秒（Flops）给出。
实测浮点峰值是指Linpack测试值，也就是说在这台机器上运行Linpack测试程序，通过各种调优方法得到的最优的测试结果。实际上在实际程序运行过程中，几乎不可能达到实测浮点峰值，更不用说达到理论浮点峰值了。这两个值只是作为衡量机器性能的一个指标，用来表明机器处理能力的一个标尺和潜能的度量。

HPL作为一个基准测试程序在HPC领域取得了巨大的成功。它对趋势进行了准确的预测，积累的优化技巧对于提高实际应用的性能发挥了重要的作用。但是，随着应用的发展，它与实际应用之间的相关性在逐渐降低，需要寻找替代者。但并不支持淘汰HPL。HPL提供给我们的历史经验数据相当的宝贵，并且HPL促进了HPC应用范围的扩大，这些重要作用都决定了HPL不会被淘汰。HPCG将会作为TOP500排名一个可供选择的基准测试，就像Green500排名一样，重新排名HPC系统。HPCG最诱人的地方就是它包含在实际应用中广泛使用的关键的通讯和计算模式，而且足够简单。HPCG不仅具有实际的数值意义，而且非常容易被大家理解和接受。我们将会在现有和即将出现的平台上，针对实际的应用程序，完成HPCG的验证和校验。验证和校验过程将会提高HPCG的质量，使之逐渐成为一个被大家广泛认可的基准测试程序。

### 二　编译安装

#### １.mpich2+GotoBLAS2+hpl

##### 1.1 mpich安装

普华服务器版本iSoftServerOS3.0_x86-64_Build4

    
    #wget http://www.mpich.org/static/downloads/1.0.8/mpich2-1.0.8.tar.gz
    #tar zxf mpich2-1.0.8.tar.gz
    #cd mpich2-1.0.8
    # ./configure
    # make
    # make install 

普华桌面版本iSoftClientOS3.0SP1

    # vi /etc/pacman.d/mirrorlist

添加SigLevel = Never”一行

修改地址“Server = http://ftp.isoft.zhcn.cc/$repo/os/$arch”

    #pacman -Rdd gcc-multilib
    #pacman -Sy gcc-fortran
    #wget http://www.mpich.org/static/downloads/1.0.8/mpich2-1.0.8.tar.gz
  
    #tar zxf mpich2-1.0.8.tar.gz
    #cd mpich2-1.0.8
    # ./configure
    # make
    # make install

##### 1.2 GotoBlas 2安装

    #wget https://www.tacc.utexas.edu/documents/1084364/1087496/GotoBLAS2-1.13.tar.gz/b58aeb8c-9d8d-4ec2-b5f1-5a5843b4d47b
    #make TARGET=NEHALEM 
    
    For PPC
    #vi cpuid_power.c 
    if (!strncasecmp(p, "POWER6", 6)) return CPUTYPE_POWER6;
    POWER6改为POWER8
    POWER6与POWER8存在差异，部分参数可能需要调整

    安装过程中需要lapack-3.1.1.tgz，如果测试机没有外网，请手动下载后放到$HOME目录
    #make TARGET=POWER BINARY=64


##### 1.3 HPL安装

    #wget http://www.netlib.org/benchmark/hpl/hpl-2.1.tar.gz
    #tar -zxf hpl-2.1.tar.gz
    #cd cd hpl-2.1
    #cp ./setup/Make.Linux_PII_FBLAS ./Make.goto
    #vi Make.goto

修改Make.goto文件
以下斜体部分是Make.goto文件的内容，//后为解释部分。

    “
    ARCH = goto        //名字需和Make.goto的尾缀名字一致
    TOPdir = /home/linpack-x86/hpl-2.1    //hpl源码所在目录
    MPdir = /home/linpack-x86/mpich2-1.0.8    //mpich2源码所在目录
    LAdir        = $(HOME)/Bench/GotoBLAS2    // GotoBlas源码的目录
    LAlib        = $(LAdir)/libgoto2.a     // GotoBlas静态库
    CC= /usr/bin/mpicc       //mpicc的绝对路径
    LINKER= /usr/bin/mpif77     //mpif77绝对路径  ”

    #make arch=Goto 

#### 2.openmpi+BLAS+hpl

##### 2.1 Openmpi安装

    #wget http://www.open-mpi.org/software/ompi/v1.7/downloads/openmpi-1.7.2.tar.bz2
    #tar xf openmpi-1.7.2.tar.bz2
    # ./configure --prefix=/shared/openmpi/openmpi-1.7.2 --enable-orterun-prefix-by-default
    # make 
    #make install

##### 2.2 BLAS

    # wget http://www.netlib.org/lapack/lapack-3.4.2.tgz
    #tar zxf lapack-3.4.2.tgz
    #cd lapack-3.4.2
    #cp INSTALL/make.inc.gfortran ./make.inc
    #make blaslib

##### 2.3 HPL
 
    wget http://www.netlib.org/benchmark/hpl/hpl.tgz
    #tar zxf hpl.tgz
    #cd hpl-2.1
    #cp setup/Make.Linux_PII_FBLAS ./Make.ompi17

修改Make.ompi17 注：部分绝对路径根据实际环境做相应调整

    "--- setup/Make.Linux_PII_FBLAS    
    +++ Make.ompi17
    @@ -64 +64 @@
    -ARCH         = Linux_PII_FBLAS
    +ARCH         = ompi17
    @@ -70 +70 @@
    -TOPdir       = $(HOME)/hpl
    +TOPdir       = /home/testtool-gcc-ppc/hpc-mpi/lapack-3.4.2/hpl-2.1
    @@ -84 +84 @@
    -MPdir        = /usr/local/mpi
    +MPdir        = /shared/openmpi/openmpi-1.7.2/
    @@ -86 +86 @@
    -MPlib        = $(MPdir)/lib/libmpich.a
    +MPlib        =
    @@ -95 +95 @@
    -LAdir        = $(HOME)/netlib/ARCHIVES/Linux_PII
    +LAdir        =/home/testtool-gcc-ppc/hpc-mpi/lapack-3.4.2
    @@ -97 +97 @@
    -LAlib        = $(LAdir)/libf77blas.a $(LAdir)/libatlas.a
    +LAlib        =  $(LAdir)/librefblas.a
    @@ -169 +169 @@
    -CC           = /usr/bin/gcc
    +CC           = /shared/openmpi/openmpi-1.7.2/bin/mpicc
    @@ -176 +176 @@
    -LINKER       = /usr/bin/g77
    +LINKER       = /shared/openmpi/openmpi-1.7.2/bin/mpif90
    "
    
    #make arch=ompi17
 
 注意：
  
    1.运行时可能会出现找到mpi库的问题，请将mpi库移动到/lib64或修改程序库搜索路径。
    
    2.如果Openmpi启动失败，可以采用mpich2+BLAS+hpl进行编译测试。

### 三.测试执行

#### 1.测试调优参数

测试参数保存在HPL.out文件，修改参数时，直接修改HPL.out文件。HPL文件内容如下，注：斜体部分为文本内容，//号为解释部分

    HPL.out      output file name (if any)  //输出文件名，同下一行一起设定
    
    6            device out (6=stdout,7=stderr,file)  //6标准输出，7标准错误输出，file可以设定为保存至特定文件，文件名上一行设定
    
    4            # of problems sizes (N)//选择测试规模的数量
    
    29 30 34 35  Ns   //计算规模的大小,一般规模越大，得出的结果越优，但规模越大，占用的内存也就越大，对系统造成的影响也越明显。所以，一般根据实际物理内存大小进行计算：N*N*8=内存总量*0.8
    
    1            # of NBs //分块矩阵的数量
    
    1 2 3 4      NBs //分块矩阵的大小，HPL采用分块矩阵的算法。分块的大小对性能有很大的影响，NB的选择和软硬件许多因素密切相关。NB值的选择主要是通过实际测试得到最优值，一般在256以下，NB × 8最好为
    Cache line的倍数。

    0 PMAP process mapping (0=Row-,1=Column-major)  //选择处理器阵列是列的排列方式还是按行的排列方式。按列的排列方式适用于节点数较多、每个节点内CPU数较少的系统；而按行的排列方式适用于节点数较少、每个节点内CPU数较多系统。在机群系统上，按列的排列方式的性能远好于按行的排列方式。
    
    1            # of process grids (P x Q) //选择测试线程类型的数量

    1 1 4        Ps

    1 4 1        Qs    //二维处理器网格（P × Q），P × Q = 系统CPU数 = 进程数。P≤Q，一般来说，P的值尽量取得小一点，因为列向通信量（通信次数和通信数据量）要远大于横向通信（和实际CPU物理阵列有关）。P = 2n，即P最好选择2的幂。HPL中，L分解的列向通信采用二元交换法（Binary Exchange），当列向处理器个数P为2的幂时，性能最优。例如，当系统进程数为4的时候，P × Q选择为1 × 4的效果要比选择2 × 2好一些。 这主要取决于CPU物理互联网络。在集群测试中，P × Q  系统CPU总核数

    16.0         threshold   //测试的精度，保持默认即可

    1            # of panel fact  //选PFACTS算法的方式，可以进行试验进行选择

    0 1 2        PFACTs (0=left, 1=Crout, 2=Right)// 用PFACTs算法对nn列作消元

    2            # of recursive stopping criterium //选择NBMINS的值，递归L分解

    2 4          NBMINs (>= 1) //设定NBMINS的值，一般推荐2、4、8

    1            # of panels in recursion  //选择NDIVS的值

    2            NDIVs  //设定NDIVS的值

    1            # of recursive panel fact.  //选择PFACTs算法

    0 1 2        RFACTs (0=left, 1=Crout, 2=Right)


    1            # of broadcast    //选择L的横向广播方式


    0            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)  // HPL提供六种广播方式。其中前四种适合于快速网络；后两种采用将数据切割后传送的方式，主要适合于速度较慢的 网络。前四种算法，分别采用单环/双环、第一列处理器不优先/优先。 01234567t=0t=1t=2t=3t=4t=5t=6t=70)Increasing-ring:单环,不优先01234567t=0,1t= 2t=3t=4t=5t=6t=71)Increasing-ring(M):单环,优先01234567t=0,1t=1t=2t=3t=2t=3t= 4t=52)Increasing-2-ring:双环,不优先01234567t=0,1,2t=2t=3t=3t=4t=5t=63) Increasing-2-ring(M):双环,优先。对于系统规模较小、处理器数（进程数）较少的系统来说，这四个选择对性能影响很小。对于横向处理器数Q较大的网络来来说，选择双环可以减少横向通信宽度，较小横向通信延迟。另外，第一列处理器优先算法也可以确保下一次L分解的尽早开始。在小规模系统中，一般选择0或1；对于大规模系统，一般选3 


    1            # of lookahead depth //选择横向通信的通信深度


    0            DEPTHs (>=0)  // DEPTHs＝0表明将L一次性广播出去，也就是将整个L分解完成以后在一次性广播； DEPTHs＝1表示将L分两次广播；依此类推.小规模系统中，DEPTHs一般选择1或2；对于大规模系统，选择2～5之间


    2            SWAP (0=bin-exch,1=long,2=mix) 


    64           swapping threshold  // U的广播为列向广播，HPL共提供了三种U的广播算法：二元交换（Binary Exchange）法、Long法和二者混合法。SWAP=“0”，采用二元交换法；SWAP=“1”，采用Long法；SWAP=“2”，采用混合法。二元交换法的通信开销为㏒2P×（Latency＋NB×LocQ（N）/Bandwith），适用于通信量较小的情况；Long法的通信开销为（㏒2P＋ P－1）×Latency＋K×NB×LocQ（N）/Bandwith，适用于通信量较大的情况。其中P为列向处理器数，Latency为网络延迟， Bandwith为网络带宽，K为常数，其经验值约为2.4。LocQ（N）＝NB×NN为通信量，NN随着求解过程的进行逐步减少。由于NN在求解过程中在不断的变化，为了充分发挥两种算法的优势，HPL提供了混合法，当NN≤swapping threshold（第27行指定）时，采用二元交换；否则采用Long法。一般来说，我们选择混合法，阈值可通过公式求得一个大概值。对于小规模系统来说，此值性能影响不大，采用其缺省值即可。


    0            L1 in (0=transposed,1=no-transposed) form  //选择L的数据存放方式


    0          U  in (0=transposed,1=no-transposed) form   //选择U的数据存放格式。C语言矩阵在内存是按行存放的，Fortran语言是按列存放的。由于HPL采用C语言书写，而调用的BLAS库有可能采用C语言，也有可能采用Fortran语言编写。若选择“transposed”，则采用按列存放，否则按行存放。可以根据实际测试进行选择。


    1            Equilibration (0=no,1=yes)  //选择是否回代，默认即可


    8            memory alignment in double (> 0)  // 内存分配中作地址对齐,根据实际测试结果选择

#### 2.单核单线程

    #cd /home/linpack-x86/hpl-2.1/bin/goto
    #vi HPL.dat

设置PQ值为1x1

物理内存为8G，设置规模大小为28800

其余值可以保持默认，测试时，可以根据实际物理平台，进行试验，进行选择。

    #./xhpl
    
#### 3.多核多线程

    # echo secretword=abc123 > /etc/mpd.conf

    #chmod 600 /etc/mpd.conf 

    #mpd&

    注：普华服务器系统iSoftServerOS3.0_x86-64_Build4 启动mpd时会出现以下提示：

    “/usr/local/bin/mpdlib.py:8: DeprecationWarning: The popen2 module is deprecated.  Use the subprocess module.
    import sys, os, signal, popen2, socket, select, inspect

    /usr/local/bin/mpdlib.py:15: DeprecationWarning: the md5 module is deprecated; use hashlib instead 
from  md5       import  new as md5new”

应该为python版本问题造成，可以降低python版本或忽略该问题。

CPU实际的物理核数为4.修改hpl文件中的PQ值为1x4

    #mpirun -np 4 ./xhpl 

#### 4.大页模式测试

    #umount /mnt
    #mount -t hugetlbfs none /mnt
    #echo 3 > /proc/sys/vm/drop_caches
    #cat /proc/meminfo | grep Hugepagesize // iSoftServerOS3.0_x86-64_Build4的单页大小为2048KB
    #echo 3276> /proc/sys/vm/nr_hugepages //实际物理内存为8G，大页面数量应该为8Gx0.8x1024/2=3276
    # echo secretword=abc123 > /etc/mpd.conf
    #chmod 600 /etc/mpd.conf 
    #mpd&

CPU实际的物理核数为4.修改hpl文件中的PQ值为1x4
修改规模大小：N*N*8=3276*2*1024*1024*0.8
N设定为26112

    #mpirun -np 4 ./xhpl 

注：此处调用的xhpl，需对hpl代码进行修改，使其调用hugpage

#### 5.集群测试

    #echo secretword=abc123 > /etc/mpd.conf
    #chmod 600 /etc/mpd.conf
    注：每个Node都必须添加该文件，并且正确设置该文件的权限

下面以2个节点进行测试说明。
设定每个节点的名字

临时修改hostname方法：

设定主节点为node0

    #hostname node0

设定第1个次节点为node1
    
    #hostname node1

永久修改hostname方法：

iSoftServerOS3.0_x86-64_Build4 平台

    # vi /etc/sysconfig/network
    修改HOSTNAME=node0

iSoftClientOS3.0 SP1 平台

    # vi /etc/hostname
    输入 node0

设置主机名与ip地址映射
    
    #vi /etc/hosts

主节点node0 添加如下：

    “192.168.32.165 node0
    192.168.32.171 node1”

Node1添加如下：

    “192.168.32.165 node0
    192.168.32.171 node1”

设置免密码登录

    #ssh-keygen -t rsa

连续点击enter键

将主节点node0的公钥文件复制到集群的node1节点上：
    
    #scp /root/.ssh/id_rsa.pub root@node1:/root

登录到节点node1上，将主节点的公钥信息追加到/root/.ssh/authorized_keys文件中，并设置authorized_keys的权限，顺序执行以下两条命令：

    [root@node1 ~]#cat id_rsa.pub >> .ssh/authorized_keys 
    [root@node1 ~]#chmod 600 .ssh/authorized_keys

Node1，方法同步骤（1）-（3），即在每个节点上都生成一对公私密钥，并且将各自的公钥信息追加到主节点的/root/.ssh/authorized_keys文件中。 
设置完成后验证一下节点之间是否可以实现无密码登录

    #ssh root@node1 

无需输入密码，可以直接登录。

将主节点node0系统中的hpl文件复制到其他节点相同目录位置

[root@node0 home]#scp -r linpack-x86 root@node1:/home/

在主节点的hpl目录下新建mpd.hosts文件

内容如下：
“node1
node2”

分别在各节点启动mpd

    #mpd &

主节点启动集群系统

    [root@node0 goto]#mpdboot -n 2 -f mpd.hosts 

运行测试

2个节点总核数为8，PQ值设定为8*1

规模大小以集群总内存进行计算，设定为29184

注:如果内存不一致，一般以最小内存的倍数进行计算

    [root@node0 goto]#mpiexec -n 8 ./xhpl

注意确保网络连通，如果出现网络断开，则测试失败。

#### 4.测试结果分析

测试结果的常见输出信息如下：

    ================================================================================
    HPLinpack 2.1  --  High-Performance Linpack benchmark  --   October 26, 2012
    Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
    Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
    Modified by Julien Langou, University of Colorado Denver
    ================================================================================
    
    An explanation of the input/output parameters follows:
    T/V    : Wall time / encoded variant.
    N      : The order of the coefficient matrix A.
    NB     : The partitioning blocking factor.
    P      : The number of process rows.
    Q      : The number of process columns.
    Time   : Time in seconds to solve the linear system.
    Gflops : Rate of execution for solving the linear system.
    
    The following parameter values will be used:
    
    N      :   17280 
    NB     :     256 
    PMAP   : Row-major process mapping
    P      :       1 
    Q      :       1 
    PFACT  :    Left 
    NBMIN  :       2 
    NDIV   :       2 
    RFACT  :    Left 
    BCAST  :   1ring 
    DEPTH  :       0 
    SWAP   : Mix (threshold = 64)
    L1     : transposed form
    U      : transposed form
    EQUIL  : yes
    ALIGN  : 8 double precision words
    
    --------------------------------------------------------------------------------
    
    - The matrix A is randomly generated for each test.
    - The following scaled residual check will be computed:
          ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
    - The relative machine precision (eps) is taken to be               1.110223e-16
    - Computational tests pass if scaled residuals are less than                16.0
    
    ================================================================================
    T/V                N    NB     P     Q               Time                 Gflops
    --------------------------------------------------------------------------------
    WR00L2L2       17280   256     1     1              75.77              4.541e+01  //测试结果
    HPL_pdgesv() start time Sun Jan  4 09:33:11 2015
    
    HPL_pdgesv() end time   Sun Jan  4 09:34:27 2015
    
    --------------------------------------------------------------------------------
    ||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=        0.0050103 ...... PASSED
    ================================================================================
    
    Finished      1 tests with the following results:
                  1 tests completed and passed residual checks,
                  0 tests completed and failed residual checks,
                  0 tests skipped because of illegal input values.
    --------------------------------------------------------------------------------
    
    End of Tests.
    ================================================================================

其中4.541e+01Gflops为测试结果，表示Linpack测试浮点值为45.41 Gflops

由此可以计算该系统的效率。以此次测试为例：

该平台所用CPU为 Intel(R) Core(TM) i5-2400 CPU @ 3.10GHz

为4核CPU。

INTELcore系列CPU每个时钟周期执行浮点运算的次数一般为8次，CPU的主频为3.1G。可以计算得出该CPU的理论浮点值为 3.1*4*8=99.2G

所以该平台的系统的效率为45.41/99.2约45.8%。

### 附1HPCG（高性能共轭梯度基准测试）

普通编译

    #wget http://www.hpcg-benchmark.org/downloads/hpcg-2.4.tar.gz
    #tar zxf hpcg-2.4.tar.gz
    #cd hpcg-2.4
    #make arch=GCC_OMP
    #cd bin

hpcg.dat 保持默认即可 
    
    #./xhpcg

多核编译

    #wget  http://www.mpich.org/static/downloads/3.1.3/mpich-3.1.3.tar.gz
    #tar zxf mpich-3.1.3.tar.gz
    #cd mpich-3.1.3
    #./configure
    #make
    #make install
    #wget http://www.hpcg-benchmark.org/downloads/hpcg-2.4.tar.gz
    #tar zxf hpcg-2.4.tar.gz
    #cd hpcg-2.4
    #make  arch=Linux_MPI
    #cd bin/
    #echo secretword=abc123 > /etc/mpd.conf
    #chmod 600 /etc/mpd.conf 
    #mpd&

hpcg.dat 保持默认即可。

    #mpirun -np 2 ./xhpcg

### 附2. Intel MPI 和 已编译好的linpack测试

    解压缩 tar –zxvf l_mpi_p_3.2.011.tgz    

    创建安装目录 mkdir /Clusterr/intelmpi ,拷贝license 到解压缩的目录中   

    执行 ./install.sh  

    root@mng l_mpi_p_3.1.038]# ./install.sh

安装完intel mpi后，使用intel编译好的linpack测试包  将l_lpk_p_10.0.2.010.tgz解压缩 

    tar –zxvf l_lpk_p_10.0.2.010.tgz    

测试方法同上面。

Top[^]()
